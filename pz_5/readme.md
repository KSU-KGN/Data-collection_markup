*Данная секция временная, задания сдаю с опазданием, если вы проверите, то буду благодарна. Спасибо!* <br>
_В программах есть комментарии_ <br>
_Начало временной секции_ <br>
### Выполнение практического задания № 4
### Урок 4. Парсинг HTML. XPath

pz_4.py  - программа <br>
pz_4.csv - данные <br>

#### Задания:

1.  Выберите веб-сайт с табличными данными, который вас интересует. (Например, news.mail.ru)
2.  Напишите код Python, использующий библиотеку requests для отправки HTTP GET-запроса на сайт 
и получения HTML-содержимого страницы.
3.  Выполните парсинг содержимого HTML с помощью библиотеки lxml, чтобы извлечь данные из таблицы.
4.  Сохраните извлеченные данные в CSV-файл с помощью модуля csv.

Ваш код должен включать следующее:

*   Строку агента пользователя в заголовке HTTP-запроса, чтобы имитировать веб-браузер и избежать блокировки сервером.
*   Выражения XPath для выбора элементов данных таблицы и извлечения их содержимого.
*   Обработка ошибок для случаев, когда данные не имеют ожидаемого формата.
*   Комментарии для объяснения цели и логики кода.

Примечание: Пожалуйста, не забывайте соблюдать этические и юридические нормы при веб-скреппинге.

_В программах есть комментарии_ <br>
_Конец временной секции_ <br>
*Данная секция временная, задания сдаю с опазданием, если вы проверите, то буду благодарна. Спасибо!*

### Выполнение практического задания № 5
### Урок 5. Scrapy

*Работаю над Вариантом I, затем планирую сделать и Вариант II* <br>
hhru.py    - код Scrapy Spider (в процессе разработки, выкладываю рабочую версию) <br>
hhru.json  - пример выходных данных (в процессе, пока не структурированы) <br>

#### Задания:

##### I вариант:

Создать по материалам семинара на базе сайта hh.ru БД вакансий и доработать её (формат данных и т.д.) <br>
Т.е. выполнить все пункты Варианта II на базе сайта hh.ru.

##### II вариант:

1.  Найдите сайт, содержащий интересующий вас список или каталог. (Например, сайт книгами labirint.ru) <br>
Это может быть список книг, фильмов, спортивных команд или что-то еще, что вас заинтересовало.
2.  Создайте новый проект Scrapy и определите нового паука. <br>
С помощью атрибута start_urls укажите URL выбранной вами веб-страницы.
3.  Определите метод парсинга для извлечения интересующих вас данных. <br>
Используйте селекторы XPath или CSS для навигации по HTML и извлечения данных. <br>
Возможно, потребуется извлечь данные с нескольких страниц или перейти по ссылкам на другие страницы.
4.  Сохраните извлеченные данные в структурированном формате. <br>
Вы можете использовать оператор yield для возврата данных из паука, <br>
которые Scrapy может записать в файл в выбранном вами формате (например, JSON или CSV).
6.  Конечным результатом работы должен быть код Scrapy Spider, а также пример выходных данных. <br>

Не забывайте соблюдать правила robots.txt и условия обслуживания веб-сайта, <br>
а также ответственно подходите к использованию веб-скрейпинга.
