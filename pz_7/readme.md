*Данная секция временная, задания сдаю с опазданием, если вы проверите, то буду благодарна. Спасибо!* <br>
_В программах есть комментарии_ <br>
_Начало временной секции_ <br>

### Выполнение практического задания № 6
### Урок 6. Scrapy. Парсинг фото и файлов

*Задание выполнено полностью:* <br>
items.py	- элемент (Item) в Scrapy <br>
pict.csv	- дополнительные сведения об изображениях <br>

#### Задания:

1.  Создайте новый проект Scrapy. <br> 
Дайте ему подходящее имя и убедитесь, что ваше окружение правильно настроено для работы с проектом.
2.  Создайте нового паука, способного перемещаться по сайту www.unsplash.com. <br>
Ваш паук должен уметь перемещаться по категориям фотографий и получать доступ к страницам отдельных фотографий.
3.  Определите элемент (Item) в Scrapy, который будет представлять изображение. <br>
Ваш элемент должен включать такие детали, <br>
как URL изображения, название изображения и категорию, к которой оно принадлежит.
4.  Используйте Scrapy ImagesPipeline для загрузки изображений. <br>
Обязательно установите параметр IMAGES_STORE в файле settings.py. <br>
Убедитесь, что ваш паук правильно выдает элементы изображений, которые может обработать ImagesPipeline.
5.  Сохраните дополнительные сведения об изображениях (название, категория) в CSV-файле. <br>
Каждая строка должна соответствовать одному изображению и содержать <br>
URL изображения, локальный путь к файлу (после загрузки), название и категорию.

_В программах есть комментарии_ <br>
_Конец временной секции_ <br>
*Данная секция временная, задания сдаю с опазданием, если вы проверите, то буду благодарна. Спасибо!*

### Выполнение практического задания № 7
### Урок 7. Selenium в Python

letters.py	- Python-скрипт (в процессе разработки) <br>
letters.csv	- краткий отчет (в процессе) <br>

#### Задания:

1.  Выберите веб-сайт, который содержит информацию, представляющую интерес для извлечения данных.  <br>
Это может быть новостной сайт, платформа для электронной коммерции или любой другой сайт,  <br>
который позволяет осуществлять скрейпинг (убедитесь в соблюдении условий обслуживания сайта).
2.  Используя Selenium, напишите сценарий для автоматизации процесса перехода на нужную страницу сайта.  
3.  Определите элементы HTML, содержащие информацию, которую вы хотите извлечь  <br>
(например, заголовки статей, названия продуктов, цены и т.д.).
4.  Используйте BeautifulSoup для парсинга содержимого HTML и извлечения нужной информации из идентифицированных элементов.
5.  Обработайте любые ошибки или исключения, которые могут возникнуть в процессе скрейпинга.
6.  Протестируйте свой скрипт на различных сценариях, чтобы убедиться, что он точно извлекает нужные данные.
7.  Предоставьте ваш Python-скрипт вместе с кратким отчетом (не более 1 страницы), который включает следующее:  <br>
**URL сайта.** Укажите URL сайта, который вы выбрали для анализа.  <br>
**Описание.** Предоставьте краткое описание информации, которую вы хотели извлечь из сайта.  <br>
**Подход.** Объясните подход, который вы использовали для навигации по сайту, определения соответствующих элементов и извлечения нужных данных.  <br>
**Трудности.** Опишите все проблемы и препятствия, с которыми вы столкнулись в ходе реализации проекта, и как вы их преодолели.  <br>
**Результаты.** Включите образец извлеченных данных в выбранном вами структурированном формате (например, CSV или JSON).  <br>

Примечание: Обязательно соблюдайте условия обслуживания сайта и избегайте чрезмерного скрейпинга,  <br>
который может нарушить нормальную работу сайта.
